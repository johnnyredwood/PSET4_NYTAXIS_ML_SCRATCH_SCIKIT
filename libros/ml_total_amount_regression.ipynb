{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b5ed615-db94-42fa-b437-f64433a0f2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.2.1\n",
      "Collecting snowflake-connector-python\n",
      "  Downloading snowflake_connector_python-4.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python)\n",
      "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: cryptography>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (41.0.4)\n",
      "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (23.2.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.8.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2023.3.post1)\n",
      "Requirement already satisfied: requests<3.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.31.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (23.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions<5,>=4.3 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (4.8.0)\n",
      "Collecting filelock<4,>=3.5 (from snowflake-connector-python)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.11.0)\n",
      "Collecting tomlkit (from snowflake-connector-python)\n",
      "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting boto3>=1.24 (from snowflake-connector-python)\n",
      "  Downloading boto3-1.40.69-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting botocore>=1.24 (from snowflake-connector-python)\n",
      "  Downloading botocore-1.40.69-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.24->snowflake-connector-python)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3>=1.24->snowflake-connector-python)\n",
      "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore>=1.24->snowflake-connector-python) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore>=1.24->snowflake-connector-python) (2.0.7)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.11/site-packages (from cryptography>=3.1.0->snowflake-connector-python) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=3.1.0->snowflake-connector-python) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python) (1.16.0)\n",
      "Downloading snowflake_connector_python-4.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading boto3-1.40.69-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.40.69-py3-none-any.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: asn1crypto, tomlkit, jmespath, filelock, botocore, s3transfer, boto3, snowflake-connector-python\n",
      "Successfully installed asn1crypto-1.5.1 boto3-1.40.69 botocore-1.40.69 filelock-3.20.0 jmespath-1.0.1 s3transfer-0.14.0 snowflake-connector-python-4.0.0 tomlkit-0.13.3\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install snowflake-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3469558c-3018-4f83-938d-d5ca57f19c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row \n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7825713a-617b-44d2-8951-3f38cd27843f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PORT_POSTGRES: 5432\n",
      "POSTGRES_DB: ny_taxi\n",
      "POSTGRES_USER: usuario_spark\n",
      "POSTGRES_PASSWORD set: True\n"
     ]
    }
   ],
   "source": [
    "#Carga de datos obt + filtrados\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(f\"PORT_POSTGRES: {os.getenv('PORT_POSTGRES')}\")\n",
    "print(f\"POSTGRES_DB: {os.getenv('POSTGRES_DB')}\")\n",
    "print(f\"POSTGRES_USER: {os.getenv('POSTGRES_USER')}\")\n",
    "print(f\"POSTGRES_PASSWORD set: {bool(os.getenv('POSTGRES_PASSWORD'))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e69b1187-8a37-4c9e-a903-f6235a0102f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jar_path = \"/home/jovyan/work/postgresql-42.2.5.jar\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ML_Desde_Postgres\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars\", jar_path) \\\n",
    "    .config(\"spark.driver.extraClassPath\", jar_path) \\\n",
    "    .config(\"spark.executor.extraClassPath\", jar_path) \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fb564a8-df9c-4ce5-9ac2-96e8df48d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:postgresql://warehouses:5432/{os.getenv('POSTGRES_DB')}\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", \"(SELECT * FROM analytics.obt_trips WHERE year = 2022 LIMIT 1000000) AS t1\") \\\n",
    "    .option(\"user\", os.getenv('POSTGRES_USER')) \\\n",
    "    .option(\"password\", os.getenv('POSTGRES_PASSWORD')) \\\n",
    "    .option(\"fetchsize\", \"100000\").load()\n",
    "\n",
    "df_validation = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:postgresql://warehouses:5432/{os.getenv('POSTGRES_DB')}\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", \"(SELECT * FROM analytics.obt_trips WHERE year = 2023 LIMIT 200000) AS t2\") \\\n",
    "    .option(\"user\", os.getenv('POSTGRES_USER')) \\\n",
    "    .option(\"password\", os.getenv('POSTGRES_PASSWORD')) \\\n",
    "    .option(\"fetchsize\", \"100000\").load()\n",
    "\n",
    "df_testing = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:postgresql://warehouses:5432/{os.getenv('POSTGRES_DB')}\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", \"(SELECT * FROM analytics.obt_trips WHERE year = 2024 LIMIT 200000) AS t3\") \\\n",
    "    .option(\"user\", os.getenv('POSTGRES_USER')) \\\n",
    "    .option(\"password\", os.getenv('POSTGRES_PASSWORD')) \\\n",
    "    .option(\"fetchsize\", \"100000\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64a37bf8-017c-4f4d-8180-23488cd31251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_tabla_con_validaciones(df_obt):\n",
    "\n",
    "    print(\"Iniciando proceso de carga de datos de la OBT desde Schema Analytics para su validacion\")\n",
    "\n",
    "    try:\n",
    "\n",
    "        df_obt_sin_nulos = df_obt.filter(F.col(\"DO_LOCATION_ID\").isNotNull() & F.col(\"PASSENGER_COUNT\").isNotNull() & F.col(\"PAYMENT_TYPE\").isNotNull() & F.col(\"PU_LOCATION_ID\").isNotNull() & F.col(\"RATE_CODE_ID\").isNotNull() & F.col(\"DROPOFF_DATETIME\").isNotNull() & F.col(\"PICKUP_DATETIME\").isNotNull() & F.col(\"TRIP_DISTANCE\").isNotNull() & F.col(\"VENDOR_ID\").isNotNull())\n",
    "\n",
    "        df_con_datos_coherentes = df_obt_sin_nulos.filter((F.col(\"PASSENGER_COUNT\")>0) & (F.col(\"PASSENGER_COUNT\")<10) & (F.col(\"EXTRA\")>=0) & (F.col(\"FARE_AMOUNT\")>=0) & (F.col(\"TIP_AMOUNT\")>=0) & (F.col(\"TOLLS_AMOUNT\")>=0) & (F.col(\"TOTAL_AMOUNT\")>=0) & (F.col(\"TRIP_DISTANCE\")>0) & (F.col(\"TRIP_DURATION_MIN\")>1) & (F.col(\"TRIP_DURATION_MIN\")<180) & (F.col(\"AVG_SPEED_MPH\")>0) & (F.col(\"AVG_SPEED_MPH\")<100) & (F.col(\"TIP_PCT\")>=0) & (F.col(\"PU_LOCATION_ID\").between(1, 265)) & (F.col(\"DO_LOCATION_ID\").between(1, 265)))\n",
    "        \n",
    "        df_con_fechas_coherentes= df_con_datos_coherentes.filter((F.col(\"MONTH\")>0) & (F.col(\"MONTH\")<13) & (F.col(\"YEAR\")>=2022) & (F.col(\"YEAR\")<=2024))\n",
    "\n",
    "        print(\"Tabla OBT con validaciones generada correctamente\")\n",
    "        \n",
    "        return df_con_fechas_coherentes\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo generar la tabla OBT de Taxis con validaciones: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcb774a4-bf24-4d0c-ac27-bf283a4fcac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso de carga de datos de la OBT desde Schema Analytics para su validacion\n",
      "Tabla OBT con validaciones generada correctamente\n",
      "Iniciando proceso de carga de datos de la OBT desde Schema Analytics para su validacion\n",
      "Tabla OBT con validaciones generada correctamente\n",
      "Iniciando proceso de carga de datos de la OBT desde Schema Analytics para su validacion\n",
      "Tabla OBT con validaciones generada correctamente\n"
     ]
    }
   ],
   "source": [
    "df_obt_training= generar_tabla_con_validaciones(df_training)\n",
    "df_obt_validation= generar_tabla_con_validaciones(df_validation)\n",
    "df_obt_testing= generar_tabla_con_validaciones(df_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ac5b9be-03c3-4161-92f3-22a14dc1bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "feature_columns = [\n",
    "    'pickup_datetime', 'pickup_hour', 'pickup_dow', 'month', 'year',\n",
    "    'pu_location_id', 'pu_zone', 'pu_borough',\n",
    "    'service_type', 'vendor_id', 'vendor_name', 'rate_code_id', 'rate_code_desc',\n",
    "    'payment_type', 'payment_type_desc', 'trip_type',\n",
    "    'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', \n",
    "    'tip_amount', 'tolls_amount', 'improvement_surcharge', 'congestion_surcharge', \n",
    "    'airport_fee', 'store_and_fwd_flag'\n",
    "]\n",
    "\n",
    "target_column = 'total_amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8eb9c40-2418-42f8-a1f8-b35d713ae12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_prepared = df_obt_training.select(feature_columns + [target_column])\n",
    "df_validation_prepared = df_obt_validation.select(feature_columns + [target_column])\n",
    "df_testing_prepared = df_obt_testing.select(feature_columns + [target_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8ef1b28-4bde-44f4-8d89-301daefdc06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features numéricas: ['trip_distance', 'passenger_count', 'pickup_hour', 'pickup_dow', 'month', 'year', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'congestion_surcharge', 'airport_fee']\n",
      "Features categóricas: ['service_type', 'vendor_name', 'rate_code_desc', 'pu_borough', 'payment_type_desc']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "final_numeric_features = [\n",
    "    'trip_distance', \n",
    "    'passenger_count', \n",
    "    'pickup_hour', \n",
    "    'pickup_dow', \n",
    "    'month', \n",
    "    'year',\n",
    "    'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount',\n",
    "    'improvement_surcharge', 'congestion_surcharge', 'airport_fee'\n",
    "]\n",
    "\n",
    "final_categorical_features = [\n",
    "    'service_type',\n",
    "    'vendor_name', \n",
    "    'rate_code_desc',\n",
    "    'pu_borough',\n",
    "    'payment_type_desc'\n",
    "]\n",
    "\n",
    "print(\"Features numéricas:\", final_numeric_features)\n",
    "print(\"Features categóricas:\", final_categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1545c449-ba5b-44ed-92d0-45f2e8c8a75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después de eliminar nulos:\n",
      "Train: (27, 28), Val: (187469, 28), Test: (180168, 28)\n",
      "\n",
      "Después del preprocesamiento:\n",
      "X_train: (27, 23)\n",
      "X_val: (187469, 23)\n",
      "X_test: (180168, 23)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pandas as pd\n",
    "\n",
    "train_pd= df_training_prepared.toPandas()\n",
    "val_pd= df_validation_prepared.toPandas()\n",
    "test_pd= df_testing_prepared.toPandas()\n",
    "\n",
    "train_clean = train_pd.dropna(subset=final_numeric_features + final_categorical_features)\n",
    "val_clean = val_pd.dropna(subset=final_numeric_features + final_categorical_features)\n",
    "test_clean = test_pd.dropna(subset=final_numeric_features + final_categorical_features)\n",
    "\n",
    "print(f\"Después de eliminar nulos:\")\n",
    "print(f\"Train: {train_clean.shape}, Val: {val_clean.shape}, Test: {test_clean.shape}\")\n",
    "\n",
    "X_train = train_clean[final_numeric_features + final_categorical_features]\n",
    "y_train = train_clean[target_column]\n",
    "\n",
    "X_val = val_clean[final_numeric_features + final_categorical_features]\n",
    "y_val = val_clean[target_column]\n",
    "\n",
    "X_test = test_clean[final_numeric_features + final_categorical_features]\n",
    "y_test = test_clean[target_column]\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), final_numeric_features),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), final_categorical_features)\n",
    "])\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "feature_names = (\n",
    "    final_numeric_features + \n",
    "    list(preprocessor.named_transformers_['cat'].get_feature_names_out(final_categorical_features))\n",
    ")\n",
    "\n",
    "X_train_processed = pd.DataFrame(X_train_processed, columns=feature_names)\n",
    "X_val_processed = pd.DataFrame(X_val_processed, columns=feature_names)\n",
    "X_test_processed = pd.DataFrame(X_test_processed, columns=feature_names)\n",
    "\n",
    "print(f\"\\nDespués del preprocesamiento:\")\n",
    "print(f\"X_train: {X_train_processed.shape}\")\n",
    "print(f\"X_val: {X_val_processed.shape}\")\n",
    "print(f\"X_test: {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28c5e33b-104f-4ad5-8bc0-d2c66df7f4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución del target:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'float' and 'decimal.Decimal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistribución del target:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain - Media: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_train\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Std: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal - Media: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_val\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Std: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_val\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest - Media: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_test\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Std: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_test\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py:11424\u001b[0m, in \u001b[0;36mNDFrame._add_numeric_operations.<locals>.std\u001b[0;34m(self, axis, skipna, ddof, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11405\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[1;32m  11406\u001b[0m     _num_ddof_doc,\n\u001b[1;32m  11407\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn sample standard deviation over requested axis.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11422\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  11423\u001b[0m ):\n\u001b[0;32m> 11424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNDFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py:11137\u001b[0m, in \u001b[0;36mNDFrame.std\u001b[0;34m(self, axis, skipna, ddof, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstd\u001b[39m(\n\u001b[1;32m  11130\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  11131\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11135\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  11136\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m> 11137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stat_function_ddof\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  11138\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m  11139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py:11101\u001b[0m, in \u001b[0;36mNDFrame._stat_function_ddof\u001b[0;34m(self, name, func, axis, skipna, ddof, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m  11099\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stat_axis_number\n\u001b[0;32m> 11101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  11102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddof\u001b[49m\n\u001b[1;32m  11103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/series.py:4670\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   4665\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   4666\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not allow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwd_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumeric_only\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4667\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-numeric dtypes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4668\u001b[0m     )\n\u001b[1;32m   4669\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 4670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelegate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/nanops.py:158\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    156\u001b[0m         result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/nanops.py:940\u001b[0m, in \u001b[0;36mnanstd\u001b[0;34m(values, axis, skipna, ddof, mask)\u001b[0m\n\u001b[1;32m    937\u001b[0m orig_dtype \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    938\u001b[0m values, mask, _, _, _ \u001b[38;5;241m=\u001b[39m _get_values(values, skipna, mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[0;32m--> 940\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mnanvar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrap_results(result, orig_dtype)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/nanops.py:96\u001b[0m, in \u001b[0;36mdisallow.__call__.<locals>._f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 96\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# we want to transform an object array\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# ValueError message to the more typical TypeError\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# e.g. this is normally a disallowed function on\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# object arrays that contain strings\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(args[\u001b[38;5;241m0\u001b[39m]):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/nanops.py:158\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    156\u001b[0m         result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/nanops.py:1007\u001b[0m, in \u001b[0;36mnanvar\u001b[0;34m(values, axis, skipna, ddof, mask)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1006\u001b[0m     avg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(avg, axis)\n\u001b[0;32m-> 1007\u001b[0m sqr \u001b[38;5;241m=\u001b[39m _ensure_numeric((\u001b[43mavg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1009\u001b[0m     np\u001b[38;5;241m.\u001b[39mputmask(sqr, mask, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'decimal.Decimal'"
     ]
    }
   ],
   "source": [
    "print(f\"Distribución del target:\")\n",
    "print(f\"Train - Media: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")\n",
    "print(f\"Val - Media: {y_val.mean():.2f}, Std: {y_val.std():.2f}\")\n",
    "print(f\"Test - Media: {y_test.mean():.2f}, Std: {y_test.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a3fa72-cb70-4bda-b7a7-9344479582a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = X_train_processed.values\n",
    "X_val_np = X_val_processed.values\n",
    "X_test_np = X_test_processed.values\n",
    "\n",
    "y_train_np = y_train.values\n",
    "y_val_np = y_val.values  \n",
    "y_test_np = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e64dcd8-1e75-437f-89d0-82fa621a24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name=\"\"):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R2: {r2:.4f}\")\n",
    "    return rmse, mae, r2\n",
    "\n",
    "y_train_mean = np.mean(y_train_np)\n",
    "y_val_baseline = np.full_like(y_val_np, y_train_mean)\n",
    "y_test_baseline = np.full_like(y_test_np, y_train_mean)\n",
    "\n",
    "print(\"Baseline\")\n",
    "rmse_base_val, mae_base_val, r2_base_val = evaluate_model(y_val_np, y_val_baseline, \"Val Baseline\")\n",
    "rmse_base_test, mae_base_test, r2_base_test = evaluate_model(y_test_np, y_test_baseline, \"Test Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c7904-7f74-405c-8283-e994f6c043b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "X_train_poly = poly.fit_transform(X_train_np)\n",
    "X_val_poly = poly.transform(X_val_np)\n",
    "X_test_poly = poly.transform(X_test_np)\n",
    "\n",
    "print(f\"Con Polynomial Features: {X_train_poly.shape}\")\n",
    "\n",
    "class SGDRegressorScratch:\n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000, alpha=0.0, tol=1e-4, random_state=42):\n",
    "        self.lr = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.alpha = alpha\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.random_state)\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.random.normal(0, 0.01, n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            total_loss = 0\n",
    "            for j in range(n_samples):\n",
    "                y_pred = np.dot(X_shuffled[j], self.weights) + self.bias\n",
    "                \n",
    "                error = y_pred - y_shuffled[j]\n",
    "                dw = error * X_shuffled[j] + self.alpha * self.weights\n",
    "                db = error\n",
    "                \n",
    "                self.weights -= self.lr * dw\n",
    "                self.bias -= self.lr * db\n",
    "                \n",
    "                total_loss += error ** 2\n",
    "            \n",
    "            avg_loss = total_loss / n_samples\n",
    "            if i > 0 and abs(avg_loss - prev_loss) < self.tol:\n",
    "                print(f\"Converge en la iteracion {i}\")\n",
    "                break\n",
    "            prev_loss = avg_loss\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "class RidgeRegressionScratch:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        A = np.dot(X.T, X) + self.alpha * np.eye(n_features)\n",
    "        b = np.dot(X.T, y)\n",
    "        self.weights = np.linalg.solve(A, b)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights)\n",
    "\n",
    "class LassoRegressionScratch:\n",
    "    def __init__(self, alpha=1.0, max_iter=1000, tol=1e-4):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        \n",
    "    def soft_threshold(self, rho, lamda):\n",
    "        if rho < -lamda:\n",
    "            return rho + lamda\n",
    "        elif rho > lamda:\n",
    "            return rho - lamda\n",
    "        else:\n",
    "            return 0\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            weights_old = self.weights.copy()\n",
    "            \n",
    "            for j in range(n_features):\n",
    "                y_pred = np.dot(X, self.weights)\n",
    "                rho_j = np.dot(X[:, j], y - y_pred + self.weights[j] * X[:, j])\n",
    "                \n",
    "                self.weights[j] = self.soft_threshold(rho_j, self.alpha * n_samples) / np.dot(X[:, j], X[:, j])\n",
    "            \n",
    "            if np.max(np.abs(self.weights - weights_old)) < self.tol:\n",
    "                print(f\"Converge en la iteracion {iteration}\")\n",
    "                break\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights)\n",
    "\n",
    "\n",
    "print(\"Modelos Scratch\")\n",
    "\n",
    "models_scratch = {\n",
    "    'SGD_Scratch': SGDRegressorScratch(learning_rate=0.001, max_iter=1000, alpha=0.01),\n",
    "    'Ridge_Scratch': RidgeRegressionScratch(alpha=1.0),\n",
    "    'Lasso_Scratch': LassoRegressionScratch(alpha=0.1, max_iter=1000)\n",
    "}\n",
    "\n",
    "results_scratch = {}\n",
    "\n",
    "for name, model in models_scratch.items():\n",
    "    print(f\"\\nEntrenando {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.fit(X_train_poly, y_train_np)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    y_val_pred = model.predict(X_val_poly)\n",
    "    y_test_pred = model.predict(X_test_poly)\n",
    "    \n",
    "    rmse_val, mae_val, r2_val = evaluate_model(y_val_np, y_val_pred, f\"Val {name}\")\n",
    "    rmse_test, mae_test, r2_test = evaluate_model(y_test_np, y_test_pred, f\"Test {name}\")\n",
    "    \n",
    "    results_scratch[name] = {\n",
    "        'val_rmse': rmse_val, 'val_mae': mae_val, 'val_r2': r2_val,\n",
    "        'test_rmse': rmse_test, 'test_mae': mae_test, 'test_r2': r2_test,\n",
    "        'train_time': train_time,\n",
    "        'n_features': X_train_poly.shape[1],\n",
    "        'n_coef_nonzero': np.sum(np.abs(model.weights) > 1e-6) if hasattr(model, 'weights') else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c10515c-7b6c-47da-abbd-13e26d2ff636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"Modelos Scikit\")\n",
    "\n",
    "# Definir pipelines equivalentes\n",
    "pipelines = {\n",
    "    'SGD_Sklearn': Pipeline([\n",
    "        ('regressor', SGDRegressor(random_state=42))\n",
    "    ]),\n",
    "    'Ridge_Sklearn': Pipeline([\n",
    "        ('regressor', Ridge(random_state=42))\n",
    "    ]),\n",
    "    'Lasso_Sklearn': Pipeline([\n",
    "        ('regressor', Lasso(random_state=42))\n",
    "    ]),\n",
    "    'ElasticNet_Sklearn': Pipeline([\n",
    "        ('regressor', ElasticNet(random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Grid search parameters\n",
    "param_grids = {\n",
    "    'SGD_Sklearn': {\n",
    "        'regressor__alpha': [0.001, 0.01, 0.1],\n",
    "        'regressor__learning_rate': ['constant', 'adaptive'],\n",
    "        'regressor__eta0': [0.001, 0.01],\n",
    "        'regressor__max_iter': [1000, 2000]\n",
    "    },\n",
    "    'Ridge_Sklearn': {\n",
    "        'regressor__alpha': [0.1, 1.0, 10.0, 100.0]\n",
    "    },\n",
    "    'Lasso_Sklearn': {\n",
    "        'regressor__alpha': [0.001, 0.01, 0.1, 1.0]\n",
    "    },\n",
    "    'ElasticNet_Sklearn': {\n",
    "        'regressor__alpha': [0.001, 0.01, 0.1],\n",
    "        'regressor__l1_ratio': [0.2, 0.5, 0.8]\n",
    "    }\n",
    "}\n",
    "\n",
    "results_sklearn = {}\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    print(f\"\\nEntrenando {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, param_grids[name], \n",
    "        cv=3, scoring='neg_mean_squared_error', \n",
    "        n_jobs=-1, verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_poly, y_train_np)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_val_pred = best_model.predict(X_val_poly)\n",
    "    y_test_pred = best_model.predict(X_test_poly)\n",
    "    \n",
    "    rmse_val, mae_val, r2_val = evaluate_model(y_val_np, y_val_pred, f\"Val {name}\")\n",
    "    rmse_test, mae_test, r2_test = evaluate_model(y_test_np, y_test_pred, f\"Test {name}\")\n",
    "    \n",
    "    results_sklearn[name] = {\n",
    "        'val_rmse': rmse_val, 'val_mae': mae_val, 'val_r2': r2_val,\n",
    "        'test_rmse': rmse_test, 'test_mae': mae_test, 'test_r2': r2_test,\n",
    "        'train_time': train_time,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'n_coef_nonzero': np.sum(np.abs(best_model.named_steps['regressor'].coef_) > 1e-6)\n",
    "    }\n",
    "    \n",
    "    print(f\"Mejores parámetros: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5237af76-1247-42bd-8931-7f82db6ba357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Comparación Modelos\")\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "comparison_data.append({\n",
    "    'Model': 'Baseline_Mean',\n",
    "    'Type': 'Baseline',\n",
    "    'Val_RMSE': rmse_base_val,\n",
    "    'Test_RMSE': rmse_base_test,\n",
    "    'Val_MAE': mae_base_val, \n",
    "    'Test_MAE': mae_base_test,\n",
    "    'Val_R2': r2_base_val,\n",
    "    'Test_R2': r2_base_test,\n",
    "    'Train_Time': 0,\n",
    "    'Nonzero_Coeffs': 0\n",
    "})\n",
    "\n",
    "for name, results in results_scratch.items():\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Type': 'From_Scratch',\n",
    "        'Val_RMSE': results['val_rmse'],\n",
    "        'Test_RMSE': results['test_rmse'],\n",
    "        'Val_MAE': results['val_mae'],\n",
    "        'Test_MAE': results['test_mae'],\n",
    "        'Val_R2': results['val_r2'],\n",
    "        'Test_R2': results['test_r2'],\n",
    "        'Train_Time': results['train_time'],\n",
    "        'Nonzero_Coeffs': results['n_coef_nonzero'] or results['n_features']\n",
    "    })\n",
    "\n",
    "for name, results in results_sklearn.items():\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Type': 'Sklearn',\n",
    "        'Val_RMSE': results['val_rmse'],\n",
    "        'Test_RMSE': results['test_rmse'],\n",
    "        'Val_MAE': results['val_mae'],\n",
    "        'Test_MAE': results['test_mae'],\n",
    "        'Val_R2': results['val_r2'],\n",
    "        'Test_R2': results['test_r2'],\n",
    "        'Train_Time': results['train_time'],\n",
    "        'Nonzero_Coeffs': results['n_coef_nonzero']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.round(4)\n",
    "\n",
    "print(\"Resultados ordenados por RMSE de Validación:\")\n",
    "display(comparison_df.sort_values('Val_RMSE'))\n",
    "\n",
    "# Mejor modelo\n",
    "best_model_row = comparison_df.loc[comparison_df['Val_RMSE'].idxmin()]\n",
    "print(f\"Mejor Modelo: {best_model_row['Model']}\")\n",
    "print(f\"RMSE Validación: {best_model_row['Val_RMSE']:.2f}\")\n",
    "print(f\"RMSE Test: {best_model_row['Test_RMSE']:.2f}\")\n",
    "print(f\"R cuadrado Test: {best_model_row['Test_R2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37523f1c-bc6d-47ae-940e-bd7dbf3c6c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Conclusiones\")\n",
    "\n",
    "print(f\"• Mejor modelo: {best_model_row['Model']}\")\n",
    "print(f\"• Mejor RMSE test: {best_model_row['Test_RMSE']:.2f}\")\n",
    "print(f\"• Mejor R cuadrado test: {best_model_row['Test_R2']:.4f}\")\n",
    "print(f\"• Improvement vs baseline: {((rmse_base_test - best_model_row['Test_RMSE']) / rmse_base_test * 100):.1f}%\")\n",
    "\n",
    "print(f\"Observaciones:\")\n",
    "scratch_models = comparison_df[comparison_df['Type'] == 'From_Scratch']\n",
    "sklearn_models = comparison_df[comparison_df['Type'] == 'Sklearn']\n",
    "\n",
    "if not scratch_models.empty and not sklearn_models.empty:\n",
    "    avg_scratch_rmse = scratch_models['Test_RMSE'].mean()\n",
    "    avg_sklearn_rmse = sklearn_models['Test_RMSE'].mean()\n",
    "    print(f\"RMSE promedio From-Scratch: {avg_scratch_rmse:.2f}\")\n",
    "    print(f\"RMSE promedio Scikit-learn: {avg_sklearn_rmse:.2f}\")\n",
    "    print(f\"Diferencia: {avg_scratch_rmse - avg_sklearn_rmse:.2f}\")\n",
    "\n",
    "print(f\"Recomendaciones\")\n",
    "print(\"1. Reentrenar mensualmente con datos nuevos\")\n",
    "print(\"2. Monitorear drift de datos y performance\")\n",
    "print(\"3. Considerar agregar features externas (clima, eventos)\")\n",
    "print(\"4. Implementar sistema de logging de predicciones\")\n",
    "print(\"5. Establecer thresholds de alerta para degradación del modelo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d856cab2-2bbf-4008-b6e1-91d40336e6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
