{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b5ed615-db94-42fa-b437-f64433a0f2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.11/site-packages (1.2.1)\n",
      "Requirement already satisfied: snowflake-connector-python in /opt/conda/lib/python3.11/site-packages (4.0.0)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (1.5.1)\n",
      "Requirement already satisfied: cryptography>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (41.0.4)\n",
      "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (23.2.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.8.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2023.3.post1)\n",
      "Requirement already satisfied: requests<3.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.31.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (23.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions<5,>=4.3 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (4.8.0)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.20.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.11.0)\n",
      "Requirement already satisfied: tomlkit in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (0.13.3)\n",
      "Requirement already satisfied: boto3>=1.24 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (1.40.71)\n",
      "Requirement already satisfied: botocore>=1.24 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (1.40.71)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3>=1.24->snowflake-connector-python) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /opt/conda/lib/python3.11/site-packages (from boto3>=1.24->snowflake-connector-python) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore>=1.24->snowflake-connector-python) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore>=1.24->snowflake-connector-python) (2.0.7)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.11/site-packages (from cryptography>=3.1.0->snowflake-connector-python) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=3.1.0->snowflake-connector-python) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install snowflake-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3469558c-3018-4f83-938d-d5ca57f19c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row \n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7825713a-617b-44d2-8951-3f38cd27843f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PORT_POSTGRES: 5432\n",
      "POSTGRES_DB: ny_taxi\n",
      "POSTGRES_USER: usuario_spark\n",
      "POSTGRES_PASSWORD set: True\n"
     ]
    }
   ],
   "source": [
    "#Carga de datos obt + filtrados\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(f\"PORT_POSTGRES: {os.getenv('PORT_POSTGRES')}\")\n",
    "print(f\"POSTGRES_DB: {os.getenv('POSTGRES_DB')}\")\n",
    "print(f\"POSTGRES_USER: {os.getenv('POSTGRES_USER')}\")\n",
    "print(f\"POSTGRES_PASSWORD set: {bool(os.getenv('POSTGRES_PASSWORD'))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e69b1187-8a37-4c9e-a903-f6235a0102f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jar_path = \"/home/jovyan/work/postgresql-42.2.5.jar\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ML_Desde_Postgres\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars\", jar_path) \\\n",
    "    .config(\"spark.driver.extraClassPath\", jar_path) \\\n",
    "    .config(\"spark.executor.extraClassPath\", jar_path) \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fb564a8-df9c-4ce5-9ac2-96e8df48d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obt = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:postgresql://warehouses:5432/{os.getenv('POSTGRES_DB')}\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", \"(SELECT * FROM analytics.obt_trips LIMIT 100000) AS t1\") \\\n",
    "    .option(\"user\", os.getenv('POSTGRES_USER')) \\\n",
    "    .option(\"password\", os.getenv('POSTGRES_PASSWORD')) \\\n",
    "    .option(\"fetchsize\", \"100000\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64a37bf8-017c-4f4d-8180-23488cd31251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_tabla_con_validaciones(df_obt):\n",
    "\n",
    "    print(\"Iniciando proceso de carga de datos de la OBT desde Schema Analytics para su validacion\")\n",
    "\n",
    "    try:\n",
    "        df_obt_sin_nulos = df_obt.filter(F.col(\"DO_LOCATION_ID\").isNotNull() & F.col(\"PASSENGER_COUNT\").isNotNull() & F.col(\"PAYMENT_TYPE\").isNotNull() & F.col(\"PU_LOCATION_ID\").isNotNull() & F.col(\"RATE_CODE_ID\").isNotNull() & F.col(\"DROPOFF_DATETIME\").isNotNull() & F.col(\"PICKUP_DATETIME\").isNotNull() & F.col(\"TRIP_DISTANCE\").isNotNull() & F.col(\"VENDOR_ID\").isNotNull())\n",
    "\n",
    "        df_con_datos_coherentes = df_obt_sin_nulos.filter((F.col(\"PASSENGER_COUNT\")>0) & (F.col(\"PASSENGER_COUNT\")<10) & (F.col(\"EXTRA\")>=0) & (F.col(\"FARE_AMOUNT\")>=0) & (F.col(\"TIP_AMOUNT\")>=0) & (F.col(\"TOLLS_AMOUNT\")>=0) & (F.col(\"TOTAL_AMOUNT\")>=0) & (F.col(\"TRIP_DISTANCE\")>0) & (F.col(\"TRIP_DURATION_MIN\")>1) & (F.col(\"TRIP_DURATION_MIN\")<180) & (F.col(\"AVG_SPEED_MPH\")>0) & (F.col(\"AVG_SPEED_MPH\")<100) & (F.col(\"TIP_PCT\")>=0) & (F.col(\"PU_LOCATION_ID\").between(1, 265)) & (F.col(\"DO_LOCATION_ID\").between(1, 265)))\n",
    "        \n",
    "        df_con_fechas_coherentes= df_con_datos_coherentes.filter((F.col(\"MONTH\")>0) & (F.col(\"MONTH\")<13) & (F.col(\"YEAR\")>=2022) & (F.col(\"YEAR\")<=2024))\n",
    "\n",
    "        print(\"Tabla OBT con validaciones generada correctamente\")\n",
    "        \n",
    "        return df_con_fechas_coherentes\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo generar la tabla OBT de Taxis con validaciones: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcb774a4-bf24-4d0c-ac27-bf283a4fcac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso de carga de datos de la OBT desde Schema Analytics para su validacion\n",
      "Tabla OBT con validaciones generada correctamente\n"
     ]
    }
   ],
   "source": [
    "df_obt_validado= generar_tabla_con_validaciones(df_obt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ac5b9be-03c3-4161-92f3-22a14dc1bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "feature_columns = [\n",
    "    'pickup_datetime', 'pickup_hour', 'pickup_dow', 'month', 'year',\n",
    "    'pu_location_id', 'pu_zone', 'pu_borough',\n",
    "    'service_type', 'vendor_id', 'vendor_name', 'rate_code_id', 'rate_code_desc',\n",
    "    'payment_type', 'payment_type_desc', 'trip_type',\n",
    "    'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', \n",
    "    'tip_amount', 'tolls_amount', 'improvement_surcharge', 'congestion_surcharge', \n",
    "    'airport_fee', 'store_and_fwd_flag'\n",
    "]\n",
    "\n",
    "target_column = 'total_amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8eb9c40-2418-42f8-a1f8-b35d713ae12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obt_preparado = df_obt_validado.select(feature_columns + [target_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8ef1b28-4bde-44f4-8d89-301daefdc06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features numéricas: ['trip_distance', 'passenger_count', 'pickup_hour', 'pickup_dow', 'month', 'year', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'congestion_surcharge', 'airport_fee']\n",
      "Features categóricas: ['service_type', 'vendor_name', 'rate_code_desc', 'pu_borough', 'payment_type_desc']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "final_numeric_features = [\n",
    "    'trip_distance', \n",
    "    'passenger_count', \n",
    "    'pickup_hour', \n",
    "    'pickup_dow', \n",
    "    'month', \n",
    "    'year',\n",
    "    'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount',\n",
    "    'improvement_surcharge', 'congestion_surcharge', 'airport_fee'\n",
    "]\n",
    "\n",
    "final_categorical_features = [\n",
    "    'service_type',\n",
    "    'vendor_name', \n",
    "    'rate_code_desc',\n",
    "    'pu_borough',\n",
    "    'payment_type_desc'\n",
    "]\n",
    "\n",
    "print(\"Features numéricas:\", final_numeric_features)\n",
    "print(\"Features categóricas:\", final_categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1545c449-ba5b-44ed-92d0-45f2e8c8a75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después de eliminar nulos:\n",
      "DF Shape: (92476, 28)\n",
      "Shape of train_clean (60%): (55486, 28)\n",
      "Shape of val_clean (20%): (18495, 28)\n",
      "Shape of test_clean (20%): (18495, 28)\n",
      "\n",
      "Después del preprocesamiento:\n",
      "X_train: (55486, 34)\n",
      "X_val: (18495, 34)\n",
      "X_test: (18495, 34)\n",
      "   trip_distance  passenger_count  pickup_hour  pickup_dow   month  year  \\\n",
      "0       0.115957        -0.454585     0.965147   -1.536966 -0.0315   0.0   \n",
      "1      -0.243912        -0.454585     0.447490    0.434042 -0.0315   0.0   \n",
      "2      -0.643766        -0.454585    -0.932929   -1.044214 -0.0315   0.0   \n",
      "3       3.288915        -0.454585     0.102385   -1.044214 -0.0315   0.0   \n",
      "4      -0.547330         0.563378     1.137699    0.926794 -0.0315   0.0   \n",
      "\n",
      "   fare_amount     extra   mta_tax  tip_amount  ...  pu_borough_Brooklyn  \\\n",
      "0     0.107199 -0.455420  0.037035   -0.916759  ...                  0.0   \n",
      "1    -0.152263 -0.050186  0.037035    0.270296  ...                  0.0   \n",
      "2    -0.671186  1.165517  0.037035   -0.916759  ...                  0.0   \n",
      "3     3.263983 -0.860655  0.037035    3.453439  ...                  0.0   \n",
      "4    -0.584699 -0.455420  0.037035   -0.118323  ...                  0.0   \n",
      "\n",
      "   pu_borough_Manhattan  pu_borough_N/A  pu_borough_Queens  \\\n",
      "0                   1.0             0.0                0.0   \n",
      "1                   1.0             0.0                0.0   \n",
      "2                   1.0             0.0                0.0   \n",
      "3                   0.0             0.0                1.0   \n",
      "4                   1.0             0.0                0.0   \n",
      "\n",
      "   pu_borough_Staten Island  pu_borough_Unknown  payment_type_desc_Cash  \\\n",
      "0                       0.0                 0.0                     1.0   \n",
      "1                       0.0                 0.0                     0.0   \n",
      "2                       0.0                 0.0                     1.0   \n",
      "3                       0.0                 0.0                     0.0   \n",
      "4                       0.0                 0.0                     0.0   \n",
      "\n",
      "   payment_type_desc_Credit card  payment_type_desc_Dispute  \\\n",
      "0                            0.0                        0.0   \n",
      "1                            1.0                        0.0   \n",
      "2                            0.0                        0.0   \n",
      "3                            1.0                        0.0   \n",
      "4                            1.0                        0.0   \n",
      "\n",
      "   payment_type_desc_No charge  \n",
      "0                          0.0  \n",
      "1                          0.0  \n",
      "2                          0.0  \n",
      "3                          0.0  \n",
      "4                          0.0  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pandas as pd\n",
    "\n",
    "df_obt_preparado_pd= df_obt_preparado.toPandas()\n",
    "\n",
    "df_clean = df_obt_preparado_pd.dropna(subset=final_numeric_features + final_categorical_features)\n",
    "\n",
    "print(f\"Después de eliminar nulos:\")\n",
    "print(f\"DF Shape: {df_clean.shape}\")\n",
    "\n",
    "percentage1 = 0.60\n",
    "percentage2 = 0.20\n",
    "percentage3 = 0.20\n",
    "\n",
    "train_clean = df_clean.sample(frac=percentage1, random_state=42)\n",
    "remaining_df = df_clean.drop(train_clean.index)\n",
    "\n",
    "val_clean = remaining_df.sample(frac=(percentage2 / (1 - percentage1)), random_state=42)\n",
    "test_clean = remaining_df.drop(val_clean.index)\n",
    "\n",
    "print(f\"Shape of train_clean (60%): {train_clean.shape}\")\n",
    "print(f\"Shape of val_clean (20%): {val_clean.shape}\")\n",
    "print(f\"Shape of test_clean (20%): {test_clean.shape}\")\n",
    "\n",
    "X_train = train_clean[final_numeric_features + final_categorical_features]\n",
    "y_train = train_clean[target_column]\n",
    "\n",
    "X_val = val_clean[final_numeric_features + final_categorical_features]\n",
    "y_val = val_clean[target_column]\n",
    "\n",
    "X_test = test_clean[final_numeric_features + final_categorical_features]\n",
    "y_test = test_clean[target_column]\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), final_numeric_features),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), final_categorical_features)\n",
    "])\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "feature_names = (\n",
    "    final_numeric_features + \n",
    "    list(preprocessor.named_transformers_['cat'].get_feature_names_out(final_categorical_features))\n",
    ")\n",
    "\n",
    "X_train_processed = pd.DataFrame(X_train_processed, columns=feature_names)\n",
    "X_val_processed = pd.DataFrame(X_val_processed, columns=feature_names)\n",
    "X_test_processed = pd.DataFrame(X_test_processed, columns=feature_names)\n",
    "\n",
    "print(f\"\\nDespués del preprocesamiento:\")\n",
    "print(f\"X_train: {X_train_processed.shape}\")\n",
    "print(f\"X_val: {X_val_processed.shape}\")\n",
    "print(f\"X_test: {X_test_processed.shape}\")\n",
    "\n",
    "print(X_train_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28c5e33b-104f-4ad5-8bc0-d2c66df7f4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución del target:\n",
      "Train - Media: 20.86, Std: 14.42\n",
      "Val - Media: 20.87, Std: 14.32\n",
      "Test - Media: 20.80, Std: 14.31\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.astype(float)\n",
    "y_val = y_val.astype(float)\n",
    "y_test = y_test.astype(float)\n",
    "\n",
    "print(f\"Distribución del target:\")\n",
    "print(f\"Train - Media: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")\n",
    "print(f\"Val - Media: {y_val.mean():.2f}, Std: {y_val.std():.2f}\")\n",
    "print(f\"Test - Media: {y_test.mean():.2f}, Std: {y_test.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99a3fa72-cb70-4bda-b7a7-9344479582a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = X_train_processed.values\n",
    "X_val_np = X_val_processed.values\n",
    "X_test_np = X_test_processed.values\n",
    "\n",
    "y_train_np = y_train.values\n",
    "y_val_np = y_val.values  \n",
    "y_test_np = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e64dcd8-1e75-437f-89d0-82fa621a24a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline\n",
      "Val Baseline:\n",
      "RMSE: 14.32\n",
      "MAE: 9.76\n",
      "R cuadrado: -0.0000\n",
      "Test Baseline:\n",
      "RMSE: 14.31\n",
      "MAE: 9.78\n",
      "R cuadrado: -0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name=\"\"):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R cuadrado: {r2:.4f}\")\n",
    "    return rmse, mae, r2\n",
    "\n",
    "y_train_mean = np.mean(y_train_np)\n",
    "y_val_baseline = np.full_like(y_val_np, y_train_mean)\n",
    "y_test_baseline = np.full_like(y_test_np, y_train_mean)\n",
    "\n",
    "print(\"Baseline\")\n",
    "rmse_base_val, mae_base_val, r2_base_val = evaluate_model(y_val_np, y_val_baseline, \"Val Baseline\")\n",
    "rmse_base_test, mae_base_test, r2_base_test = evaluate_model(y_test_np, y_test_baseline, \"Test Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c7904-7f74-405c-8283-e994f6c043b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con Polynomial Features: (55486, 595)\n",
      "Modelos Scratch\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "X_train_poly = poly.fit_transform(X_train_np)\n",
    "X_val_poly = poly.transform(X_val_np)\n",
    "X_test_poly = poly.transform(X_test_np)\n",
    "\n",
    "print(f\"Con Polynomial Features: {X_train_poly.shape}\")\n",
    "\n",
    "scaler_poly = StandardScaler()\n",
    "X_train_poly = scaler_poly.fit_transform(X_train_poly)\n",
    "X_val_poly = scaler_poly.transform(X_val_poly)\n",
    "X_test_poly = scaler_poly.transform(X_test_poly)\n",
    "\n",
    "class SGDRegressorScratch:\n",
    "    def __init__(self, learning_rate=1e-4, alpha=0.01, max_iter=100, tol=1e-4, batch_size=64, clip_value=1e3, random_state=42):\n",
    "        self.lr = learning_rate\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.batch_size = batch_size\n",
    "        self.clip_value = clip_value\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.random_state)\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.random.normal(0, 0.01, n_features)\n",
    "        self.bias = 0.0\n",
    "        prev_loss = np.inf\n",
    "\n",
    "        for epoch in range(self.max_iter):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X, y = X[indices], y[indices]\n",
    "            total_loss = 0.0\n",
    "\n",
    "            for start in range(0, n_samples, self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                X_batch, y_batch = X[start:end], y[start:end]\n",
    "\n",
    "                y_pred = np.dot(X_batch, self.weights) + self.bias\n",
    "                errors = y_pred - y_batch\n",
    "\n",
    "                dw = np.dot(X_batch.T, errors) / len(X_batch) + self.alpha * self.weights\n",
    "                db = np.mean(errors)\n",
    "\n",
    "                dw = np.clip(dw, -self.clip_value, self.clip_value)\n",
    "                db = np.clip(db, -self.clip_value, self.clip_value)\n",
    "\n",
    "                self.weights -= self.lr * dw\n",
    "                self.bias -= self.lr * db\n",
    "\n",
    "                total_loss += np.mean(errors ** 2)\n",
    "\n",
    "            avg_loss = total_loss / (n_samples // self.batch_size)\n",
    "            if not np.isfinite(avg_loss):\n",
    "                print(f\"Overflow detectado en epoch {epoch}, deteniendo.\")\n",
    "                break\n",
    "\n",
    "            if abs(prev_loss - avg_loss) < self.tol:\n",
    "                print(f\"Convergió en epoch {epoch+1} con pérdida {avg_loss:.6f}\")\n",
    "                break\n",
    "\n",
    "            prev_loss = avg_loss\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "\n",
    "class RidgeRegressionScratch:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_features = X.shape[1]\n",
    "        A = np.dot(X.T, X) + self.alpha * np.eye(n_features)\n",
    "        b = np.dot(X.T, y)\n",
    "        self.weights = np.linalg.solve(A, b)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights)\n",
    "\n",
    "class LassoRegressionScratch:\n",
    "    def __init__(self, alpha=1.0, max_iter=1000, tol=1e-4):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def soft_threshold(self, rho, lamda):\n",
    "        if rho < -lamda:\n",
    "            return rho + lamda\n",
    "        elif rho > lamda:\n",
    "            return rho - lamda\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            weights_old = self.weights.copy()\n",
    "            for j in range(n_features):\n",
    "                y_pred = np.dot(X, self.weights)\n",
    "                rho = np.dot(X[:, j], y - y_pred + self.weights[j] * X[:, j])\n",
    "\n",
    "                denom = np.dot(X[:, j], X[:, j])\n",
    "                if denom != 0:\n",
    "                    self.weights[j] = self.soft_threshold(rho, self.alpha * n_samples) / denom\n",
    "                else:\n",
    "                    self.weights[j] = 0.0\n",
    "\n",
    "            if np.max(np.abs(self.weights - weights_old)) < self.tol:\n",
    "                break\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights)\n",
    "\n",
    "class ElasticNetScratch:\n",
    "    def __init__(self, alpha=1.0, l1_ratio=0.5, max_iter=1000, tol=1e-4, learning_rate=1e-3):\n",
    "        self.alpha = alpha\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0.0\n",
    "        prev_loss = np.inf\n",
    "\n",
    "        for it in range(self.max_iter):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X, y = X[indices], y[indices]\n",
    "            total_loss = 0.0\n",
    "\n",
    "            for i in range(n_samples):\n",
    "                y_pred = np.dot(X[i], self.weights) + self.bias\n",
    "                error = y_pred - y[i]\n",
    "\n",
    "                # L1 + L2\n",
    "                dw = error * X[i] + self.alpha * (\n",
    "                    self.l1_ratio * np.sign(self.weights) + (1 - self.l1_ratio) * self.weights\n",
    "                )\n",
    "                db = error\n",
    "\n",
    "                self.weights -= self.lr * dw\n",
    "                self.bias -= self.lr * db\n",
    "\n",
    "                total_loss += error ** 2\n",
    "\n",
    "            avg_loss = total_loss / n_samples\n",
    "            if abs(prev_loss - avg_loss) < self.tol:\n",
    "                break\n",
    "            prev_loss = avg_loss\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "def grid_search(models, param_grids, X_train, y_train, X_val, y_val):\n",
    "    results = []\n",
    "    for model_name, model_class in models.items():\n",
    "        grid = param_grids[model_name]\n",
    "        for params in product(*grid.values()):\n",
    "            params_dict = dict(zip(grid.keys(), params))\n",
    "            model = model_class(**params_dict)\n",
    "\n",
    "            start = time.time()\n",
    "            model.fit(X_train, y_train)\n",
    "            end = time.time()\n",
    "\n",
    "            preds = model.predict(X_val)\n",
    "            mse = np.mean((y_val - preds) ** 2)\n",
    "\n",
    "            results.append({\n",
    "                'model': model_name,\n",
    "                **params_dict,\n",
    "                'mse': mse,\n",
    "                'time_sec': round(end - start, 4)\n",
    "            })\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Modelos Scratch\")\n",
    "\n",
    "X_train_poly = X_train_poly.astype(float)\n",
    "y_train = y_train.astype(float)\n",
    "X_val_poly = X_val_poly.astype(float)\n",
    "y_val = y_val.astype(float)\n",
    "\n",
    "X_train = X_train_poly.values if hasattr(X_train_poly, \"values\") else X_train_poly\n",
    "y_train = y_train.values if hasattr(y_train, \"values\") else y_train\n",
    "X_val = X_val_poly.values if hasattr(X_val_poly, \"values\") else X_val_poly\n",
    "y_val = y_val.values if hasattr(y_val, \"values\") else y_val\n",
    "\n",
    "models = {\n",
    "    'SGD': SGDRegressorScratch,\n",
    "    'Ridge': RidgeRegressionScratch,\n",
    "    'Lasso': LassoRegressionScratch,\n",
    "    'ElasticNet': ElasticNetScratch\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'SGD': {'learning_rate': [1e-3, 1e-4], 'alpha': [0.0, 0.01], 'max_iter': [100]},\n",
    "    'Ridge': {'alpha': [0.1, 1.0, 10.0]},\n",
    "    'Lasso': {'alpha': [0.001, 0.01, 0.1]},\n",
    "    'ElasticNet': {'alpha': [0.01, 0.1], 'l1_ratio': [0.3, 0.7]}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "results = grid_search(models, param_grids, X_train, y_train, X_val, y_val)\n",
    "df_results = pd.DataFrame(results).sort_values(by=\"mse\")\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c10515c-7b6c-47da-abbd-13e26d2ff636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"Modelos Scikit\")\n",
    "\n",
    "# Definir pipelines equivalentes\n",
    "pipelines = {\n",
    "    'SGD_Sklearn': Pipeline([\n",
    "        ('regressor', SGDRegressor(random_state=42))\n",
    "    ]),\n",
    "    'Ridge_Sklearn': Pipeline([\n",
    "        ('regressor', Ridge(random_state=42))\n",
    "    ]),\n",
    "    'Lasso_Sklearn': Pipeline([\n",
    "        ('regressor', Lasso(random_state=42))\n",
    "    ]),\n",
    "    'ElasticNet_Sklearn': Pipeline([\n",
    "        ('regressor', ElasticNet(random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Grid search parameters\n",
    "param_grids = {\n",
    "    'SGD_Sklearn': {\n",
    "        'regressor__alpha': [0.001, 0.01, 0.1],\n",
    "        'regressor__learning_rate': ['constant', 'adaptive'],\n",
    "        'regressor__eta0': [0.001, 0.01],\n",
    "        'regressor__max_iter': [1000, 2000]\n",
    "    },\n",
    "    'Ridge_Sklearn': {\n",
    "        'regressor__alpha': [0.1, 1.0, 10.0, 100.0]\n",
    "    },\n",
    "    'Lasso_Sklearn': {\n",
    "        'regressor__alpha': [0.001, 0.01, 0.1, 1.0]\n",
    "    },\n",
    "    'ElasticNet_Sklearn': {\n",
    "        'regressor__alpha': [0.001, 0.01, 0.1],\n",
    "        'regressor__l1_ratio': [0.2, 0.5, 0.8]\n",
    "    }\n",
    "}\n",
    "\n",
    "results_sklearn = {}\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    print(f\"\\nEntrenando {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, param_grids[name], \n",
    "        cv=3, scoring='neg_mean_squared_error', \n",
    "        n_jobs=-1, verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_poly, y_train_np)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_val_pred = best_model.predict(X_val_poly)\n",
    "    y_test_pred = best_model.predict(X_test_poly)\n",
    "    \n",
    "    rmse_val, mae_val, r2_val = evaluate_model(y_val_np, y_val_pred, f\"Val {name}\")\n",
    "    rmse_test, mae_test, r2_test = evaluate_model(y_test_np, y_test_pred, f\"Test {name}\")\n",
    "    \n",
    "    results_sklearn[name] = {\n",
    "        'val_rmse': rmse_val, 'val_mae': mae_val, 'val_r2': r2_val,\n",
    "        'test_rmse': rmse_test, 'test_mae': mae_test, 'test_r2': r2_test,\n",
    "        'train_time': train_time,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'n_coef_nonzero': np.sum(np.abs(best_model.named_steps['regressor'].coef_) > 1e-6)\n",
    "    }\n",
    "    \n",
    "    print(f\"Mejores parámetros: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5237af76-1247-42bd-8931-7f82db6ba357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Comparación Modelos\")\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "comparison_data.append({\n",
    "    'Model': 'Baseline_Mean',\n",
    "    'Type': 'Baseline',\n",
    "    'Val_RMSE': rmse_base_val,\n",
    "    'Test_RMSE': rmse_base_test,\n",
    "    'Val_MAE': mae_base_val, \n",
    "    'Test_MAE': mae_base_test,\n",
    "    'Val_R2': r2_base_val,\n",
    "    'Test_R2': r2_base_test,\n",
    "    'Train_Time': 0,\n",
    "    'Nonzero_Coeffs': 0\n",
    "})\n",
    "\n",
    "for name, results in results_scratch.items():\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Type': 'From_Scratch',\n",
    "        'Val_RMSE': results['val_rmse'],\n",
    "        'Test_RMSE': results['test_rmse'],\n",
    "        'Val_MAE': results['val_mae'],\n",
    "        'Test_MAE': results['test_mae'],\n",
    "        'Val_R2': results['val_r2'],\n",
    "        'Test_R2': results['test_r2'],\n",
    "        'Train_Time': results['train_time'],\n",
    "        'Nonzero_Coeffs': results['n_coef_nonzero'] or results['n_features']\n",
    "    })\n",
    "\n",
    "for name, results in results_sklearn.items():\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Type': 'Sklearn',\n",
    "        'Val_RMSE': results['val_rmse'],\n",
    "        'Test_RMSE': results['test_rmse'],\n",
    "        'Val_MAE': results['val_mae'],\n",
    "        'Test_MAE': results['test_mae'],\n",
    "        'Val_R2': results['val_r2'],\n",
    "        'Test_R2': results['test_r2'],\n",
    "        'Train_Time': results['train_time'],\n",
    "        'Nonzero_Coeffs': results['n_coef_nonzero']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.round(4)\n",
    "\n",
    "print(\"Resultados ordenados por RMSE de Validación:\")\n",
    "display(comparison_df.sort_values('Val_RMSE'))\n",
    "\n",
    "# Mejor modelo\n",
    "best_model_row = comparison_df.loc[comparison_df['Val_RMSE'].idxmin()]\n",
    "print(f\"Mejor Modelo: {best_model_row['Model']}\")\n",
    "print(f\"RMSE Validación: {best_model_row['Val_RMSE']:.2f}\")\n",
    "print(f\"RMSE Test: {best_model_row['Test_RMSE']:.2f}\")\n",
    "print(f\"R cuadrado Test: {best_model_row['Test_R2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d856cab2-2bbf-4008-b6e1-91d40336e6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
