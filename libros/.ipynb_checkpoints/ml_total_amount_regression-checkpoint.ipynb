{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b5ed615-db94-42fa-b437-f64433a0f2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.11/site-packages (1.2.1)\n",
      "Requirement already satisfied: snowflake-connector-python in /opt/conda/lib/python3.11/site-packages (4.0.0)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (1.5.1)\n",
      "Requirement already satisfied: cryptography>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (41.0.4)\n",
      "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (23.2.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.8.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2023.3.post1)\n",
      "Requirement already satisfied: requests<3.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.31.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (23.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions<5,>=4.3 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (4.8.0)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.20.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.11.0)\n",
      "Requirement already satisfied: tomlkit in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (0.13.3)\n",
      "Requirement already satisfied: boto3>=1.24 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (1.40.71)\n",
      "Requirement already satisfied: botocore>=1.24 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (1.40.71)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3>=1.24->snowflake-connector-python) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /opt/conda/lib/python3.11/site-packages (from boto3>=1.24->snowflake-connector-python) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore>=1.24->snowflake-connector-python) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore>=1.24->snowflake-connector-python) (2.0.7)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.11/site-packages (from cryptography>=3.1.0->snowflake-connector-python) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=3.1.0->snowflake-connector-python) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install snowflake-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3469558c-3018-4f83-938d-d5ca57f19c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row \n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7825713a-617b-44d2-8951-3f38cd27843f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PORT_POSTGRES: 5432\n",
      "POSTGRES_DB: ny_taxi\n",
      "POSTGRES_USER: usuario_spark\n",
      "POSTGRES_PASSWORD set: True\n"
     ]
    }
   ],
   "source": [
    "#Carga de datos obt + filtrados\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(f\"PORT_POSTGRES: {os.getenv('PORT_POSTGRES')}\")\n",
    "print(f\"POSTGRES_DB: {os.getenv('POSTGRES_DB')}\")\n",
    "print(f\"POSTGRES_USER: {os.getenv('POSTGRES_USER')}\")\n",
    "print(f\"POSTGRES_PASSWORD set: {bool(os.getenv('POSTGRES_PASSWORD'))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e69b1187-8a37-4c9e-a903-f6235a0102f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jar_path = \"/home/jovyan/work/postgresql-42.2.5.jar\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ML_Desde_Postgres\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars\", jar_path) \\\n",
    "    .config(\"spark.driver.extraClassPath\", jar_path) \\\n",
    "    .config(\"spark.executor.extraClassPath\", jar_path) \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fb564a8-df9c-4ce5-9ac2-96e8df48d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obt = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:postgresql://warehouses:5432/{os.getenv('POSTGRES_DB')}\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", \"(SELECT * FROM analytics.obt_trips LIMIT 1000000) AS t1\") \\ #Si hacia con mas datos moria el kernel\n",
    "    .option(\"user\", os.getenv('POSTGRES_USER')) \\\n",
    "    .option(\"password\", os.getenv('POSTGRES_PASSWORD')) \\\n",
    "    .option(\"fetchsize\", \"100000\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64a37bf8-017c-4f4d-8180-23488cd31251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_tabla_con_validaciones(df_obt):\n",
    "\n",
    "    print(\"Iniciando proceso de carga de datos de la OBT desde Schema Analytics para su validacion\")\n",
    "\n",
    "    try:\n",
    "        df_obt_sin_nulos = df_obt.filter(F.col(\"DO_LOCATION_ID\").isNotNull() & F.col(\"PASSENGER_COUNT\").isNotNull() & F.col(\"PAYMENT_TYPE\").isNotNull() & F.col(\"PU_LOCATION_ID\").isNotNull() & F.col(\"RATE_CODE_ID\").isNotNull() & F.col(\"DROPOFF_DATETIME\").isNotNull() & F.col(\"PICKUP_DATETIME\").isNotNull() & F.col(\"TRIP_DISTANCE\").isNotNull() & F.col(\"VENDOR_ID\").isNotNull())\n",
    "\n",
    "        df_con_datos_coherentes = df_obt_sin_nulos.filter((F.col(\"PASSENGER_COUNT\")>0) & (F.col(\"PASSENGER_COUNT\")<10) & (F.col(\"EXTRA\")>=0) & (F.col(\"FARE_AMOUNT\")>=0) & (F.col(\"TIP_AMOUNT\")>=0) & (F.col(\"TOLLS_AMOUNT\")>=0) & (F.col(\"TOTAL_AMOUNT\")>=0) & (F.col(\"TRIP_DISTANCE\")>0) & (F.col(\"TRIP_DURATION_MIN\")>1) & (F.col(\"TRIP_DURATION_MIN\")<180) & (F.col(\"AVG_SPEED_MPH\")>0) & (F.col(\"AVG_SPEED_MPH\")<100) & (F.col(\"TIP_PCT\")>=0) & (F.col(\"PU_LOCATION_ID\").between(1, 265)) & (F.col(\"DO_LOCATION_ID\").between(1, 265)))\n",
    "        \n",
    "        df_con_fechas_coherentes= df_con_datos_coherentes.filter((F.col(\"MONTH\")>0) & (F.col(\"MONTH\")<13) & (F.col(\"YEAR\")>=2022) & (F.col(\"YEAR\")<=2024))\n",
    "\n",
    "        print(\"Tabla OBT con validaciones generada correctamente\")\n",
    "        \n",
    "        return df_con_fechas_coherentes\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo generar la tabla OBT de Taxis con validaciones: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcb774a4-bf24-4d0c-ac27-bf283a4fcac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso de carga de datos de la OBT desde Schema Analytics para su validacion\n",
      "Tabla OBT con validaciones generada correctamente\n"
     ]
    }
   ],
   "source": [
    "df_obt_validado= generar_tabla_con_validaciones(df_obt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ac5b9be-03c3-4161-92f3-22a14dc1bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "feature_columns = [\n",
    "    'pickup_datetime', 'pickup_hour', 'pickup_dow', 'month', 'year',\n",
    "    'pu_location_id', 'pu_zone', 'pu_borough',\n",
    "    'service_type', 'vendor_id', 'vendor_name', 'rate_code_id', 'rate_code_desc',\n",
    "    'payment_type', 'payment_type_desc', 'trip_type',\n",
    "    'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', \n",
    "    'tip_amount', 'tolls_amount', 'improvement_surcharge', 'congestion_surcharge', \n",
    "    'airport_fee', 'store_and_fwd_flag'\n",
    "]\n",
    "\n",
    "target_column = 'total_amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8eb9c40-2418-42f8-a1f8-b35d713ae12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obt_preparado = df_obt_validado.select(feature_columns + [target_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8ef1b28-4bde-44f4-8d89-301daefdc06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features numéricas: ['trip_distance', 'passenger_count', 'pickup_hour', 'pickup_dow', 'month', 'year', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'congestion_surcharge', 'airport_fee']\n",
      "Features categóricas: ['service_type', 'vendor_name', 'rate_code_desc', 'pu_borough', 'payment_type_desc']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "final_numeric_features = [\n",
    "    'trip_distance', \n",
    "    'passenger_count', \n",
    "    'pickup_hour', \n",
    "    'pickup_dow', \n",
    "    'month', \n",
    "    'year',\n",
    "    'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount',\n",
    "    'improvement_surcharge', 'congestion_surcharge', 'airport_fee'\n",
    "]\n",
    "\n",
    "final_categorical_features = [\n",
    "    'service_type',\n",
    "    'vendor_name', \n",
    "    'rate_code_desc',\n",
    "    'pu_borough',\n",
    "    'payment_type_desc'\n",
    "]\n",
    "\n",
    "print(\"Features numéricas:\", final_numeric_features)\n",
    "print(\"Features categóricas:\", final_categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1545c449-ba5b-44ed-92d0-45f2e8c8a75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después de eliminar nulos:\n",
      "DF Shape: (92476, 28)\n",
      "Shape of train_clean (60%): (55486, 28)\n",
      "Shape of val_clean (20%): (18495, 28)\n",
      "Shape of test_clean (20%): (18495, 28)\n",
      "\n",
      "Después del preprocesamiento:\n",
      "X_train: (55486, 34)\n",
      "X_val: (18495, 34)\n",
      "X_test: (18495, 34)\n",
      "   trip_distance  passenger_count  pickup_hour  pickup_dow   month  year  \\\n",
      "0       0.115957        -0.454585     0.965147   -1.536966 -0.0315   0.0   \n",
      "1      -0.243912        -0.454585     0.447490    0.434042 -0.0315   0.0   \n",
      "2      -0.643766        -0.454585    -0.932929   -1.044214 -0.0315   0.0   \n",
      "3       3.288915        -0.454585     0.102385   -1.044214 -0.0315   0.0   \n",
      "4      -0.547330         0.563378     1.137699    0.926794 -0.0315   0.0   \n",
      "\n",
      "   fare_amount     extra   mta_tax  tip_amount  ...  pu_borough_Brooklyn  \\\n",
      "0     0.107199 -0.455420  0.037035   -0.916759  ...                  0.0   \n",
      "1    -0.152263 -0.050186  0.037035    0.270296  ...                  0.0   \n",
      "2    -0.671186  1.165517  0.037035   -0.916759  ...                  0.0   \n",
      "3     3.263983 -0.860655  0.037035    3.453439  ...                  0.0   \n",
      "4    -0.584699 -0.455420  0.037035   -0.118323  ...                  0.0   \n",
      "\n",
      "   pu_borough_Manhattan  pu_borough_N/A  pu_borough_Queens  \\\n",
      "0                   1.0             0.0                0.0   \n",
      "1                   1.0             0.0                0.0   \n",
      "2                   1.0             0.0                0.0   \n",
      "3                   0.0             0.0                1.0   \n",
      "4                   1.0             0.0                0.0   \n",
      "\n",
      "   pu_borough_Staten Island  pu_borough_Unknown  payment_type_desc_Cash  \\\n",
      "0                       0.0                 0.0                     1.0   \n",
      "1                       0.0                 0.0                     0.0   \n",
      "2                       0.0                 0.0                     1.0   \n",
      "3                       0.0                 0.0                     0.0   \n",
      "4                       0.0                 0.0                     0.0   \n",
      "\n",
      "   payment_type_desc_Credit card  payment_type_desc_Dispute  \\\n",
      "0                            0.0                        0.0   \n",
      "1                            1.0                        0.0   \n",
      "2                            0.0                        0.0   \n",
      "3                            1.0                        0.0   \n",
      "4                            1.0                        0.0   \n",
      "\n",
      "   payment_type_desc_No charge  \n",
      "0                          0.0  \n",
      "1                          0.0  \n",
      "2                          0.0  \n",
      "3                          0.0  \n",
      "4                          0.0  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pandas as pd\n",
    "\n",
    "df_obt_preparado_pd= df_obt_preparado.toPandas()\n",
    "\n",
    "df_clean = df_obt_preparado_pd.dropna(subset=final_numeric_features + final_categorical_features)\n",
    "\n",
    "print(f\"Después de eliminar nulos:\")\n",
    "print(f\"DF Shape: {df_clean.shape}\")\n",
    "\n",
    "percentage1 = 0.60\n",
    "percentage2 = 0.20\n",
    "percentage3 = 0.20\n",
    "\n",
    "train_clean = df_clean.sample(frac=percentage1, random_state=42)\n",
    "remaining_df = df_clean.drop(train_clean.index)\n",
    "\n",
    "val_clean = remaining_df.sample(frac=(percentage2 / (1 - percentage1)), random_state=42)\n",
    "test_clean = remaining_df.drop(val_clean.index)\n",
    "\n",
    "print(f\"Shape of train_clean (60%): {train_clean.shape}\")\n",
    "print(f\"Shape of val_clean (20%): {val_clean.shape}\")\n",
    "print(f\"Shape of test_clean (20%): {test_clean.shape}\")\n",
    "\n",
    "X_train = train_clean[final_numeric_features + final_categorical_features]\n",
    "y_train = train_clean[target_column]\n",
    "\n",
    "X_val = val_clean[final_numeric_features + final_categorical_features]\n",
    "y_val = val_clean[target_column]\n",
    "\n",
    "X_test = test_clean[final_numeric_features + final_categorical_features]\n",
    "y_test = test_clean[target_column]\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), final_numeric_features),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), final_categorical_features)\n",
    "])\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "feature_names = (\n",
    "    final_numeric_features + \n",
    "    list(preprocessor.named_transformers_['cat'].get_feature_names_out(final_categorical_features))\n",
    ")\n",
    "\n",
    "X_train_processed = pd.DataFrame(X_train_processed, columns=feature_names)\n",
    "X_val_processed = pd.DataFrame(X_val_processed, columns=feature_names)\n",
    "X_test_processed = pd.DataFrame(X_test_processed, columns=feature_names)\n",
    "\n",
    "print(f\"\\nDespués del preprocesamiento:\")\n",
    "print(f\"X_train: {X_train_processed.shape}\")\n",
    "print(f\"X_val: {X_val_processed.shape}\")\n",
    "print(f\"X_test: {X_test_processed.shape}\")\n",
    "\n",
    "print(X_train_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28c5e33b-104f-4ad5-8bc0-d2c66df7f4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución del target:\n",
      "Train - Media: 20.86, Std: 14.42\n",
      "Val - Media: 20.87, Std: 14.32\n",
      "Test - Media: 20.80, Std: 14.31\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.astype(np.float32)\n",
    "y_val = y_val.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "print(f\"Distribución del target:\")\n",
    "print(f\"Train - Media: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")\n",
    "print(f\"Val - Media: {y_val.mean():.2f}, Std: {y_val.std():.2f}\")\n",
    "print(f\"Test - Media: {y_test.mean():.2f}, Std: {y_test.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99a3fa72-cb70-4bda-b7a7-9344479582a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = X_train_processed.values\n",
    "X_val_np = X_val_processed.values\n",
    "X_test_np = X_test_processed.values\n",
    "\n",
    "y_train_np = y_train.values\n",
    "y_val_np = y_val.values  \n",
    "y_test_np = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e64dcd8-1e75-437f-89d0-82fa621a24a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline\n",
      "Val Baseline:\n",
      "RMSE: 14.32\n",
      "MAE: 9.76\n",
      "R cuadrado: -0.0000\n",
      "Test Baseline:\n",
      "RMSE: 14.31\n",
      "MAE: 9.78\n",
      "R cuadrado: -0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name=\"\"):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R cuadrado: {r2:.4f}\")\n",
    "    return rmse, mae, r2\n",
    "\n",
    "y_train_mean = np.mean(y_train_np)\n",
    "y_val_baseline = np.full_like(y_val_np, y_train_mean)\n",
    "y_test_baseline = np.full_like(y_test_np, y_train_mean)\n",
    "\n",
    "print(\"Baseline\")\n",
    "rmse_base_val, mae_base_val, r2_base_val = evaluate_model(y_val_np, y_val_baseline, \"Val Baseline\")\n",
    "rmse_base_test, mae_base_test, r2_base_test = evaluate_model(y_test_np, y_test_baseline, \"Test Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c25c7904-7f74-405c-8283-e994f6c043b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con Polynomial Features: (55486, 595)\n",
      "Modelos Scratch\n",
      "         model  learning_rate  alpha  max_iter          mse  time_sec  \\\n",
      "3          SGD         0.0001  0.010     100.0     1.853734  335.0014   \n",
      "2          SGD         0.0001  0.000     100.0     2.065233  396.7648   \n",
      "1          SGD         0.0010  0.010     100.0    96.210672  367.5094   \n",
      "11  ElasticNet            NaN  0.100       NaN   369.790232   46.9151   \n",
      "9   ElasticNet            NaN  0.010       NaN   369.861856   43.8928   \n",
      "10  ElasticNet            NaN  0.100       NaN   369.866887   50.4890   \n",
      "8   ElasticNet            NaN  0.010       NaN   369.877099   43.7915   \n",
      "0          SGD         0.0010  0.000     100.0   414.112553  420.1475   \n",
      "7        Lasso            NaN  0.010       NaN   435.021673  362.4172   \n",
      "6        Lasso            NaN  0.001       NaN   435.812490  372.4333   \n",
      "5        Ridge            NaN  1.000       NaN   436.331922    1.2097   \n",
      "4        Ridge            NaN  0.100       NaN  2008.447950    1.6891   \n",
      "\n",
      "    l1_ratio  \n",
      "3        NaN  \n",
      "2        NaN  \n",
      "1        NaN  \n",
      "11       0.7  \n",
      "9        0.7  \n",
      "10       0.3  \n",
      "8        0.3  \n",
      "0        NaN  \n",
      "7        NaN  \n",
      "6        NaN  \n",
      "5        NaN  \n",
      "4        NaN  \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "X_train_poly = poly.fit_transform(X_train_np)\n",
    "X_val_poly = poly.transform(X_val_np)\n",
    "X_test_poly = poly.transform(X_test_np)\n",
    "\n",
    "print(f\"Con Polynomial Features: {X_train_poly.shape}\")\n",
    "\n",
    "scaler_poly = StandardScaler()\n",
    "X_train_poly = scaler_poly.fit_transform(X_train_poly)\n",
    "X_val_poly = scaler_poly.transform(X_val_poly)\n",
    "X_test_poly = scaler_poly.transform(X_test_poly)\n",
    "\n",
    "class SGDRegressorScratch:\n",
    "    def __init__(self, learning_rate=1e-4, alpha=0.01, max_iter=100, tol=1e-4, batch_size=64, clip_value=1e3, random_state=42):\n",
    "        self.lr = learning_rate\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.batch_size = batch_size\n",
    "        self.clip_value = clip_value\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.random_state)\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.random.normal(0, 0.01, n_features)\n",
    "        self.bias = 0.0\n",
    "        prev_loss = np.inf\n",
    "\n",
    "        for epoch in range(self.max_iter):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X, y = X[indices], y[indices]\n",
    "            total_loss = 0.0\n",
    "\n",
    "            for start in range(0, n_samples, self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                X_batch, y_batch = X[start:end], y[start:end]\n",
    "\n",
    "                y_pred = np.dot(X_batch, self.weights) + self.bias\n",
    "                errors = y_pred - y_batch\n",
    "\n",
    "                dw = np.dot(X_batch.T, errors) / len(X_batch) + self.alpha * self.weights\n",
    "                db = np.mean(errors)\n",
    "\n",
    "                dw = np.clip(dw, -self.clip_value, self.clip_value)\n",
    "                db = np.clip(db, -self.clip_value, self.clip_value)\n",
    "\n",
    "                self.weights -= self.lr * dw\n",
    "                self.bias -= self.lr * db\n",
    "\n",
    "                total_loss += np.mean(errors ** 2)\n",
    "\n",
    "            avg_loss = total_loss / (n_samples // self.batch_size)\n",
    "            if not np.isfinite(avg_loss):\n",
    "                print(f\"Overflow detectado en epoch {epoch}, deteniendo.\")\n",
    "                break\n",
    "\n",
    "            if abs(prev_loss - avg_loss) < self.tol:\n",
    "                print(f\"Convergió en epoch {epoch+1} con pérdida {avg_loss:.6f}\")\n",
    "                break\n",
    "\n",
    "            prev_loss = avg_loss\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "\n",
    "class RidgeRegressionScratch:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_features = X.shape[1]\n",
    "        A = np.dot(X.T, X) + self.alpha * np.eye(n_features)\n",
    "        b = np.dot(X.T, y)\n",
    "        self.weights = np.linalg.solve(A, b)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights)\n",
    "\n",
    "class LassoRegressionScratch:\n",
    "    def __init__(self, alpha=1.0, max_iter=100, tol=1e-4):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def soft_threshold(self, rho, lamda):\n",
    "        return np.sign(rho) * np.maximum(np.abs(rho) - lamda, 0.0)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "\n",
    "        X_T = X.T\n",
    "        X_sq_sum = np.sum(X ** 2, axis=0)\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            weights_old = self.weights.copy()\n",
    "            y_pred = X @ self.weights\n",
    "\n",
    "            for j in range(n_features):\n",
    "                residual = y - (y_pred - X[:, j] * self.weights[j])\n",
    "                rho = np.dot(X[:, j], residual)\n",
    "                if X_sq_sum[j] != 0:\n",
    "                    self.weights[j] = self.soft_threshold(rho, self.alpha * n_samples) / X_sq_sum[j]\n",
    "                    # Actualizar solo el cambio local en y_pred\n",
    "                    y_pred += X[:, j] * (self.weights[j] - weights_old[j])\n",
    "\n",
    "            if np.max(np.abs(self.weights - weights_old)) < self.tol:\n",
    "                print(f\"Lasso convergió en {iteration+1} iteraciones.\")\n",
    "                break\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.weights\n",
    "\n",
    "\n",
    "class ElasticNetScratch:\n",
    "    def __init__(self, alpha=1.0, l1_ratio=0.5, max_iter=100, tol=1e-4, lr=1e-3):\n",
    "        self.alpha = alpha\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.lr = lr\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0.0\n",
    "        prev_loss = np.inf\n",
    "\n",
    "        for it in range(self.max_iter):\n",
    "            y_pred = X @ self.weights + self.bias\n",
    "            errors = y_pred - y\n",
    "\n",
    "            grad_w = (X.T @ errors) / n_samples\n",
    "            grad_b = np.mean(errors)\n",
    "\n",
    "            l1_grad = self.l1_ratio * np.sign(self.weights)\n",
    "            l2_grad = (1 - self.l1_ratio) * self.weights\n",
    "            grad_total = grad_w + self.alpha * (l1_grad + l2_grad)\n",
    "\n",
    "            self.weights -= self.lr * grad_total\n",
    "            self.bias -= self.lr * grad_b\n",
    "\n",
    "            loss = np.mean(errors ** 2)\n",
    "            if abs(prev_loss - loss) < self.tol:\n",
    "                print(f\"ElasticNet convergió en {it+1} iteraciones.\")\n",
    "                break\n",
    "            prev_loss = loss\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.weights + self.bias\n",
    "\n",
    "def grid_search(models, param_grids, X_train, y_train, X_val, y_val):\n",
    "    results = []\n",
    "    for model_name, model_class in models.items():\n",
    "        grid = param_grids[model_name]\n",
    "        for params in product(*grid.values()):\n",
    "            params_dict = dict(zip(grid.keys(), params))\n",
    "            model = model_class(**params_dict)\n",
    "\n",
    "            start = time.time()\n",
    "            model.fit(X_train, y_train)\n",
    "            end = time.time()\n",
    "\n",
    "            preds = model.predict(X_val)\n",
    "            mse = np.mean((y_val - preds) ** 2)\n",
    "\n",
    "            results.append({\n",
    "                'model': model_name,\n",
    "                **params_dict,\n",
    "                'mse': mse,\n",
    "                'time_sec': round(end - start, 4)\n",
    "            })\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Modelos Scratch\")\n",
    "\n",
    "X_train_poly = X_train_poly.astype(np.float32)\n",
    "X_val_poly = X_val_poly.astype(np.float32)\n",
    "\n",
    "X_train = X_train_poly.values if hasattr(X_train_poly, \"values\") else X_train_poly\n",
    "X_val = X_val_poly.values if hasattr(X_val_poly, \"values\") else X_val_poly\n",
    "\n",
    "models = {\n",
    "    'SGD': SGDRegressorScratch,\n",
    "    'Ridge': RidgeRegressionScratch,\n",
    "    'Lasso': LassoRegressionScratch,\n",
    "    'ElasticNet': ElasticNetScratch\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'SGD': {'learning_rate': [1e-3, 1e-4], 'alpha': [0.0, 0.01], 'max_iter': [100]},\n",
    "    'Ridge': {'alpha': [0.1, 1.0]},\n",
    "    'Lasso': {'alpha': [0.001, 0.01]},\n",
    "    'ElasticNet': {'alpha': [0.01, 0.1], 'l1_ratio': [0.3, 0.7]}\n",
    "}\n",
    "\n",
    "results = grid_search(models, param_grids, X_train, y_train_np, X_val, y_val_np)\n",
    "df_results = pd.DataFrame(results).sort_values(by=\"mse\")\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c10515c-7b6c-47da-abbd-13e26d2ff636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelos Scikit\n",
      "\n",
      "Entrenando SGD_Sklearn...\n",
      "Val SGD_Sklearn:\n",
      "RMSE: 1.47\n",
      "MAE: 0.39\n",
      "R cuadrado: 0.9895\n",
      "Test SGD_Sklearn:\n",
      "RMSE: 0.83\n",
      "MAE: 0.38\n",
      "R cuadrado: 0.9966\n",
      "Mejores parámetros: {'regressor__alpha': 0.1, 'regressor__eta0': 0.001, 'regressor__learning_rate': 'adaptive', 'regressor__max_iter': 2000}\n",
      "\n",
      "Entrenando Ridge_Sklearn...\n",
      "Val Ridge_Sklearn:\n",
      "RMSE: 0.13\n",
      "MAE: 0.00\n",
      "R cuadrado: 0.9999\n",
      "Test Ridge_Sklearn:\n",
      "RMSE: 0.13\n",
      "MAE: 0.00\n",
      "R cuadrado: 0.9999\n",
      "Mejores parámetros: {'regressor__alpha': 0.1}\n",
      "\n",
      "Entrenando Lasso_Sklearn...\n",
      "Val Lasso_Sklearn:\n",
      "RMSE: 0.02\n",
      "MAE: 0.01\n",
      "R cuadrado: 1.0000\n",
      "Test Lasso_Sklearn:\n",
      "RMSE: 0.02\n",
      "MAE: 0.01\n",
      "R cuadrado: 1.0000\n",
      "Mejores parámetros: {'regressor__alpha': 0.001}\n",
      "\n",
      "Entrenando ElasticNet_Sklearn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.477e+03, tolerance: 1.154e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val ElasticNet_Sklearn:\n",
      "RMSE: 0.02\n",
      "MAE: 0.01\n",
      "R cuadrado: 1.0000\n",
      "Test ElasticNet_Sklearn:\n",
      "RMSE: 0.02\n",
      "MAE: 0.01\n",
      "R cuadrado: 1.0000\n",
      "Mejores parámetros: {'regressor__alpha': 0.001, 'regressor__l1_ratio': 0.8}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"Modelos Scikit\")\n",
    "\n",
    "# Definir pipelines equivalentes\n",
    "pipelines = {\n",
    "    'SGD_Sklearn': Pipeline([\n",
    "        ('regressor', SGDRegressor(random_state=42))\n",
    "    ]),\n",
    "    'Ridge_Sklearn': Pipeline([\n",
    "        ('regressor', Ridge(random_state=42))\n",
    "    ]),\n",
    "    'Lasso_Sklearn': Pipeline([\n",
    "        ('regressor', Lasso(random_state=42))\n",
    "    ]),\n",
    "    'ElasticNet_Sklearn': Pipeline([\n",
    "        ('regressor', ElasticNet(random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Grid search parameters\n",
    "param_grids = {\n",
    "    'SGD_Sklearn': {\n",
    "        'regressor__alpha': [0.001, 0.01, 0.1],\n",
    "        'regressor__learning_rate': ['constant', 'adaptive'],\n",
    "        'regressor__eta0': [0.001, 0.01],\n",
    "        'regressor__max_iter': [1000, 2000]\n",
    "    },\n",
    "    'Ridge_Sklearn': {\n",
    "        'regressor__alpha': [0.1, 1.0, 10.0, 100.0]\n",
    "    },\n",
    "    'Lasso_Sklearn': {\n",
    "        'regressor__alpha': [0.001, 0.01, 0.1, 1.0]\n",
    "    },\n",
    "    'ElasticNet_Sklearn': {\n",
    "        'regressor__alpha': [0.001, 0.01, 0.1],\n",
    "        'regressor__l1_ratio': [0.2, 0.5, 0.8]\n",
    "    }\n",
    "}\n",
    "\n",
    "results_sklearn = {}\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    print(f\"\\nEntrenando {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, param_grids[name], \n",
    "        cv=3, scoring='neg_mean_squared_error', \n",
    "        n_jobs=-1, verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "\n",
    "    X_test_poly = X_test_poly.astype(float)\n",
    "    X_test = X_test_poly.values if hasattr(X_test_poly, \"values\") else X_test_poly\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    rmse_val, mae_val, r2_val = evaluate_model(y_val_np, y_val_pred, f\"Val {name}\")\n",
    "    rmse_test, mae_test, r2_test = evaluate_model(y_test_np, y_test_pred, f\"Test {name}\")\n",
    "    \n",
    "    results_sklearn[name] = {\n",
    "        'val_rmse': rmse_val, 'val_mae': mae_val, 'val_r2': r2_val,\n",
    "        'test_rmse': rmse_test, 'test_mae': mae_test, 'test_r2': r2_test,\n",
    "        'train_time': train_time,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'n_coef_nonzero': np.sum(np.abs(best_model.named_steps['regressor'].coef_) > 1e-6)\n",
    "    }\n",
    "    \n",
    "    print(f\"Mejores parámetros: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5237af76-1247-42bd-8931-7f82db6ba357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparación de Modelos\n",
      "\n",
      "Resultados ordenados por RMSE de Validación:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Type</th>\n",
       "      <th>Val_RMSE</th>\n",
       "      <th>Test_RMSE</th>\n",
       "      <th>Val_MAE</th>\n",
       "      <th>Test_MAE</th>\n",
       "      <th>Val_R2</th>\n",
       "      <th>Test_R2</th>\n",
       "      <th>Train_Time</th>\n",
       "      <th>Nonzero_Coeffs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Lasso_Sklearn</td>\n",
       "      <td>Sklearn</td>\n",
       "      <td>0.0233</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>571.4129</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ElasticNet_Sklearn</td>\n",
       "      <td>Sklearn</td>\n",
       "      <td>0.0234</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>648.0391</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ridge_Sklearn</td>\n",
       "      <td>Sklearn</td>\n",
       "      <td>0.1299</td>\n",
       "      <td>0.1255</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>18.1259</td>\n",
       "      <td>556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SGD_Sklearn</td>\n",
       "      <td>Sklearn</td>\n",
       "      <td>1.4699</td>\n",
       "      <td>0.8344</td>\n",
       "      <td>0.3864</td>\n",
       "      <td>0.3835</td>\n",
       "      <td>0.9895</td>\n",
       "      <td>0.9966</td>\n",
       "      <td>653.0991</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline_Mean</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>14.3174</td>\n",
       "      <td>14.3088</td>\n",
       "      <td>9.7574</td>\n",
       "      <td>9.7779</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model</td>\n",
       "      <td>From_Scratch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>learning_rate</td>\n",
       "      <td>From_Scratch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alpha</td>\n",
       "      <td>From_Scratch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max_iter</td>\n",
       "      <td>From_Scratch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mse</td>\n",
       "      <td>From_Scratch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>time_sec</td>\n",
       "      <td>From_Scratch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>l1_ratio</td>\n",
       "      <td>From_Scratch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model          Type  Val_RMSE  Test_RMSE  Val_MAE  Test_MAE  \\\n",
       "10       Lasso_Sklearn       Sklearn    0.0233     0.0155   0.0070    0.0068   \n",
       "11  ElasticNet_Sklearn       Sklearn    0.0234     0.0152   0.0063    0.0060   \n",
       "9        Ridge_Sklearn       Sklearn    0.1299     0.1255   0.0022    0.0019   \n",
       "8          SGD_Sklearn       Sklearn    1.4699     0.8344   0.3864    0.3835   \n",
       "0        Baseline_Mean      Baseline   14.3174    14.3088   9.7574    9.7779   \n",
       "1                model  From_Scratch       NaN        NaN      NaN       NaN   \n",
       "2        learning_rate  From_Scratch       NaN        NaN      NaN       NaN   \n",
       "3                alpha  From_Scratch       NaN        NaN      NaN       NaN   \n",
       "4             max_iter  From_Scratch       NaN        NaN      NaN       NaN   \n",
       "5                  mse  From_Scratch       NaN        NaN      NaN       NaN   \n",
       "6             time_sec  From_Scratch       NaN        NaN      NaN       NaN   \n",
       "7             l1_ratio  From_Scratch       NaN        NaN      NaN       NaN   \n",
       "\n",
       "    Val_R2  Test_R2  Train_Time  Nonzero_Coeffs  \n",
       "10  1.0000   1.0000    571.4129              64  \n",
       "11  1.0000   1.0000    648.0391              92  \n",
       "9   0.9999   0.9999     18.1259             556  \n",
       "8   0.9895   0.9966    653.0991             470  \n",
       "0  -0.0000  -0.0000      0.0000               0  \n",
       "1      NaN      NaN         NaN               0  \n",
       "2      NaN      NaN         NaN               0  \n",
       "3      NaN      NaN         NaN               0  \n",
       "4      NaN      NaN         NaN               0  \n",
       "5      NaN      NaN         NaN               0  \n",
       "6      NaN      NaN         NaN               0  \n",
       "7      NaN      NaN         NaN               0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mejor Modelo: Lasso_Sklearn\n",
      "RMSE Validación: 0.02\n",
      "RMSE Test: 0.02\n",
      "R cuadrado Test: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Comparación de Modelos\")\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "comparison_data.append({\n",
    "    'Model': 'Baseline_Mean',\n",
    "    'Type': 'Baseline',\n",
    "    'Val_RMSE': rmse_base_val,\n",
    "    'Test_RMSE': rmse_base_test,\n",
    "    'Val_MAE': mae_base_val,\n",
    "    'Test_MAE': mae_base_test,\n",
    "    'Val_R2': r2_base_val,\n",
    "    'Test_R2': r2_base_test,\n",
    "    'Train_Time': 0,\n",
    "    'Nonzero_Coeffs': 0\n",
    "})\n",
    "\n",
    "for name, results in df_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Type': 'From_Scratch',\n",
    "        'Val_RMSE': results.get('val_rmse', None),\n",
    "        'Test_RMSE': results.get('test_rmse', None),\n",
    "        'Val_MAE': results.get('val_mae', None),\n",
    "        'Test_MAE': results.get('test_mae', None),\n",
    "        'Val_R2': results.get('val_r2', None),\n",
    "        'Test_R2': results.get('test_r2', None),\n",
    "        'Train_Time': results.get('train_time', None),\n",
    "        'Nonzero_Coeffs': results.get('n_coef_nonzero', results.get('n_features', 0))\n",
    "    })\n",
    "\n",
    "for name, results in results_sklearn.items():\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Type': 'Sklearn',\n",
    "        'Val_RMSE': results.get('val_rmse', None),\n",
    "        'Test_RMSE': results.get('test_rmse', None),\n",
    "        'Val_MAE': results.get('val_mae', None),\n",
    "        'Test_MAE': results.get('test_mae', None),\n",
    "        'Val_R2': results.get('val_r2', None),\n",
    "        'Test_R2': results.get('test_r2', None),\n",
    "        'Train_Time': results.get('train_time', None),\n",
    "        'Nonzero_Coeffs': results.get('n_coef_nonzero', 0)\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data).round(4)\n",
    "\n",
    "print(\"\\nResultados ordenados por RMSE de Validación:\")\n",
    "display(comparison_df.sort_values(by='Val_RMSE', ascending=True))\n",
    "\n",
    "if not comparison_df.empty:\n",
    "    best_model_row = comparison_df.loc[comparison_df['Val_RMSE'].idxmin()]\n",
    "    print(f\"\\nMejor Modelo: {best_model_row['Model']}\")\n",
    "    print(f\"RMSE Validación: {best_model_row['Val_RMSE']:.2f}\")\n",
    "    print(f\"RMSE Test: {best_model_row['Test_RMSE']:.2f}\")\n",
    "    print(f\"R cuadrado Test: {best_model_row['Test_R2']:.4f}\")\n",
    "else:\n",
    "    print(\"No hay resultados disponibles para comparar.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "400e43a2-ea68-4cef-ba9c-c03341d5d4e8",
   "metadata": {},
   "source": [
    "Tras la evaluación de todos los modelos tanto los implementados desde cero como los basados en scikit-learn se obtuvieron los siguientes hallazgos:\n",
    "\n",
    "El mejor desempeño global corresponde al modelo Lasso_Sklearn, con un RMSE de validación de 0.0233 y un RMSE de prueba de 0.0155, alcanzando además un R2 1.0000 tanto en validación como en test.\n",
    "\n",
    "Esto indica un ajuste prácticamente perfecto entre las predicciones y los valores reales.\n",
    "\n",
    "Su número de coeficientes distintos de cero fue 64, lo que sugiere una buena capacidad de regularización y selección de características.\n",
    "\n",
    "ElasticNet_Sklearn obtuvo resultados prácticamente equivalentes (RMSE de validación 0.0234 y test 0.0152), lo que confirma la robustez del enfoque regularizado y el buen balance entre penalizaciones L1 y L2 (alpha=0.001, l1_ratio=0.8).\n",
    "\n",
    "El modelo Ridge_Sklearn también mostró un rendimiento muy alto (R2 0.9999), aunque con una cantidad mucho mayor de coeficientes distintos de cero (556), lo que implica menor sparsidad y mayor complejidad respecto a Lasso y ElasticNet.\n",
    "\n",
    "El modelo SGD_Sklearn, aunque alcanzó un R cuadrado elevado (0.9966 en test), tuvo un RMSE superior (≈1.47 en validación), indicando mayor inestabilidad o sensibilidad al proceso de optimización estocástica.\n",
    "\n",
    "El Baseline_Mean sirvió como referencia, con errores (RMSE 14.3, MAE 9.7) muy superiores, lo que demuestra que los modelos entrenados capturaron eficazmente la estructura del problema.\n",
    "\n",
    "Los modelos From_Scratch presentan valores NaN debido a que no se registraron resultados válidos o no se completó el entrenamiento o evaluación correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a25b37-a7e0-40ba-947b-974bb69942bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
